# Incident Detection

|        | Language Used        | Behavior Displayed |
| ------ | -------------------- | ------------------ |
| **Novice** | <ol><li>"Other teams (QA, customer support) will notify us of any problems."</li><li>"Problems with our service are obvious; outages are obvious to everyone."</li></ol> | <ol><li>_The service team is notified of incidents via manual, external notification mechanisms (ticketing system, phone calls, etc.)_</li><li>_No baseline metrics established; description of service level is bucketed into four broad categories ("available", "unavailable", "degraded", "answering, but unavailable")<sup>1</sup>_</li></ol> |
| **Beginner** | <ol><li>Most of the time, we’re the first to know when a service has transitioned from available to unavailable (or another state).</li><li>We’re the first to know when a service is impacted.</li></ol> | <ol><li>_External monitoring is in place to detect in real time when a service transitions between one of the four broad buckets_</li><li>_The team is notified in an automated way when monitoring detects a transition between these four buckets_</li></ol> |
| **Competent** | <ol><li>"We've detected a number of service level transitions via the monitoring of very new (and maybe very old) API endpoints; in all cases, MTTD was reduced."</li><li>We use historical data to perform manual, 'first approximation' guesses of service level changes; we're starting to communicate this information outward, potentially in ongoing discussions about SLAs."</li></ol> | <ol><li>_Historical data has been collected to establish broad baselines of acceptable service, enough to infer bands within the four buckets_</li><li>_External monitoring of infrastructure, API endpoints, and other outward-facing interfaces exists and is recorded in the (historical) monitoring system_</li></ol> |
| **Proficient** | <ol><li>"Other teams can help us monitor our own service because we've provided hooks for them to integrate within their own system."</li><li>"We prioritize feature requests and bug reports to these monitoring hooks within our development sprints and in our organizational support work; monitoring is a first-class citizen for our team, and takes precedent over the deployment of new features."</li><li>I know that specific code/infrastructure change caused this specific change in service level; here's how I know..."</li></ol> | <ol><li>_Baseline data is comprehensive enough to be able to be statistically correlated to current code state and map to code changes_</li><li>_Application internals report monitoring data to the monitoring system_</li><li>_Monitoring systems employ a deep use of statistical significance to provide proof (and disproof) of service anomalies_</li></ol> |
| **Advanced** | <ol><li>"We've decoupled the deployment of code and/or infrastructure changes, because we can roll those changes back or forward, as necessary, to automatically remediate the issue before any service level impact becomes notable."</li><li>Our team isn't being paged anymore for changes that automation can react to; our number of incidents that on-call engineers have to respond to is measurably down."</li></ol> | <ol><li>_Monitoring output is reincorporated into operational behavior in an automated fashion_</li><li>_Anomalies do not result in defined “incidents,” as operational systems can automatically react to statistically significant changes in metrics_</li></ol> |

<sup>1</sup> The distinction between “unavailable” and “answering” is in the former, the service does not respond to requests at all; in the latter, the service responds, but does not provide the requested functionality, i.e. returning HTTP 5xx response codes
